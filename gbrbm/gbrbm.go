// Package grbm provides support for Gaussian-Binary Restricted Bolztmann Machines.
package gbrbm

import (
	"encoding/json"
	"github.com/r9y9/nnet" // sigmoid, matrix
	"math"
	"math/rand"
	"os"
	"time"
)

// Gaussian-Binary Restricted Boltzmann Machines (GBRBM)
type GBRBM struct {
	W                      [][]float64 // Weight
	B                      []float64   // Bias of visible layer
	C                      []float64   // Bias of hidden layer
	NumHiddenUnits         int
	NumVisibleUnits        int
	PersistentVisibleUnits [][]float64 // used in Persistent contrastive learning
	GradW                  [][]float64
	GradB                  []float64
	GradC                  []float64
	Option                 TrainingOption
}

type TrainingOption struct {
	LearningRate         float64
	OrderOfGibbsSampling int // 1 is enough for many cases.
	UsePersistent        bool
	UseMean              bool // hack option
	Epoches              int
	MiniBatchSize        int
	L2Regularization     bool
	RegularizationRate   float64
	Monitoring           bool
}

// New creates new RBM instance. It requires input data and number of
// hidden units to initialize RBM.
func New(numVisibleUnits, numHiddenUnits int) *GBRBM {
	rbm := new(GBRBM)
	rand.Seed(time.Now().UnixNano())
	rbm.NumVisibleUnits = numVisibleUnits
	rbm.NumHiddenUnits = numHiddenUnits
	rbm.W = nnet.MakeMatrix(numHiddenUnits, numVisibleUnits)
	rbm.B = make([]float64, numVisibleUnits)
	rbm.C = make([]float64, numHiddenUnits)
	rbm.GradW = nnet.MakeMatrix(numHiddenUnits, numVisibleUnits)
	rbm.GradB = make([]float64, numVisibleUnits)
	rbm.GradC = make([]float64, numHiddenUnits)
	rbm.InitParam()
	return rbm
}

// Init performes a heuristic parameter initialization.
func (rbm *GBRBM) InitParam() {
	// Init W
	for i := 0; i < rbm.NumHiddenUnits; i++ {
		for j := 0; j < rbm.NumVisibleUnits; j++ {
			rbm.W[i][j] = 0.01 * rand.NormFloat64()
		}
	}

	// Init visible bias
	for j := 0; j < rbm.NumVisibleUnits; j++ {
		rbm.B[j] = 0.0
	}

	// Init hidden bias
	for i := 0; i < rbm.NumHiddenUnits; i++ {
		rbm.C[i] = 0.0
	}
}

// Load loads GBRBM from a dump file and return its instatnce.
func Load(filename string) (*GBRBM, error) {
	file, err := os.Open(filename)
	if err != nil {
		return nil, err
	}
	defer file.Close()

	decoder := json.NewDecoder(file)
	rbm := &GBRBM{}
	err = decoder.Decode(rbm)

	if err != nil {
		return nil, err
	}

	return rbm, nil
}

// Dump writes GBRBM parameters to file in json format.
func (rbm *GBRBM) Dump(filename string) error {
	rbm.PersistentVisibleUnits = nil
	return nnet.DumpAsJson(filename, rbm)
}

// Forward performs activity transformation from visible to hidden layer.
func (rbm *GBRBM) Forward(v []float64) []float64 {
	hidden := make([]float64, rbm.NumHiddenUnits)
	for i := 0; i < rbm.NumHiddenUnits; i++ {
		hidden[i] = rbm.P_H_Given_V(i, v)
	}
	return hidden
}

// P_H_Given_V returns p(h=1|v), the conditinal probability of activation
// of a hidden unit given a set of visible units.
func (rbm *GBRBM) P_H_Given_V(hiddenIndex int, v []float64) float64 {
	sum := 0.0
	for j := 0; j < rbm.NumVisibleUnits; j++ {
		sum += rbm.W[hiddenIndex][j] * v[j]
	}
	return nnet.Sigmoid(sum + rbm.C[hiddenIndex])
}

// Sample_H_Given_V returns sample drawen by p(h|v), where h is a binary unit.
func (rbm *GBRBM) Sample_H_Given_V(hiddenIndex int, v []float64) float64 {
	p := rbm.P_H_Given_V(hiddenIndex, v)
	if p > rand.Float64() {
		return 1.0
	} else {
		return 0.0
	}
}

// Mean_V_Given_H returns the mean of a Gaussian visible unit v ~ N(mean, sigma).
func (rbm *GBRBM) Mean_V_Given_H(visibleIndex int, h []float64) float64 {
	sum := 0.0
	for i := 0; i < rbm.NumHiddenUnits; i++ {
		sum += rbm.W[i][visibleIndex] * h[i]
	}
	return sum + rbm.B[visibleIndex]
}

// Sample_V_Given_H returns a sample generated by Gaussian distribution p(v|h).
func (rbm *GBRBM) Sample_V_Given_H(visibleIndex int, h []float64) float64 {
	return rbm.Mean_V_Given_H(visibleIndex, h) + rand.NormFloat64()*1.0
}

// Reconstruct performs reconstruction based on k-Gibbs sampling algorithm,
// where k is the number of iterations.
func (rbm *GBRBM) Reconstruct(v []float64, numSteps int, useMean bool) []float64 {
	// Initial value is set to input
	reconstructedVisible := make([]float64, len(v))
	copy(reconstructedVisible, v)

	// perform Gibbs-sampling
	for step := 0; step < numSteps; step++ {
		// 1. sample hidden units
		hiddenState := make([]float64, rbm.NumHiddenUnits)
		for i := 0; i < rbm.NumHiddenUnits; i++ {
			hiddenState[i] = rbm.Sample_H_Given_V(i, reconstructedVisible)
		}
		// 2. sample visible units
		// try to use the mean value instread if learning is unstable
		for j := 0; j < rbm.NumVisibleUnits; j++ {
			if useMean {
				reconstructedVisible[j] = rbm.Mean_V_Given_H(j, hiddenState)
			} else {
				reconstructedVisible[j] = rbm.Sample_V_Given_H(j, hiddenState)
			}
		}
	}

	return reconstructedVisible
}

// ReconstructionError returns reconstruction error.
// Use mean of Gaussian when computing reconstruction error.
func (rbm *GBRBM) ReconstructionError(data [][]float64, numSteps int) float64 {
	err := 0.0
	for _, v := range data {
		reconstructed := rbm.Reconstruct(v, numSteps, true)
		err += nnet.SquareErrBetweenTwoVector(v, reconstructed)
	}
	return 0.5 * err / float64(len(data))
}

// FreeEnergy returns F(v), the free energy of GBRBM given a visible vector v.
// It is assumed that the standard deviation equals to 1.
func (rbm *GBRBM) FreeEnergy(v []float64) float64 {
	energy := 0.0

	for j := 0; j < rbm.NumVisibleUnits; j++ {
		energy -= 0.5 * (rbm.B[j] - v[j]) * (rbm.B[j] - v[j])
	}

	for i := 0; i < rbm.NumHiddenUnits; i++ {
		sum := rbm.C[i]
		for j := 0; j < rbm.NumVisibleUnits; j++ {
			sum += rbm.W[i][j] * v[j]
		}
		energy -= math.Log(1 + math.Exp(sum))
	}

	return energy
}

func (rbm *GBRBM) UnSupervisedObjective(data [][]float64) float64 {
	subset := nnet.RandomSubset(data, 3000)
	return rbm.ReconstructionError(subset, rbm.Option.OrderOfGibbsSampling)
}

func (rbm *GBRBM) P_H_Given_V_Batch(v []float64) []float64 {
	h := make([]float64, rbm.NumHiddenUnits)
	for i := range h {
		h[i] = rbm.P_H_Given_V(i, v)
	}
	return h
}

// Gradient returns gradients of GBRBM parameters for a given (mini-batch) dataset.
func (rbm *GBRBM) Gradient(data [][]float64, miniBatchIndex int) ([][]float64, []float64, []float64) {
	gradW := nnet.MakeMatrix(rbm.NumHiddenUnits, rbm.NumVisibleUnits)
	gradB := make([]float64, rbm.NumVisibleUnits)
	gradC := make([]float64, rbm.NumHiddenUnits)

	for i, v := range data {
		// Set start state of Gibbs-sampling
		var gibbsStart []float64
		persistentIndex := i + miniBatchIndex*rbm.Option.MiniBatchSize
		if rbm.Option.UsePersistent {
			gibbsStart = rbm.PersistentVisibleUnits[persistentIndex]
		} else {
			gibbsStart = v
		}

		// Perform reconstruction using Gibbs-sampling
		reconstructedVisible := rbm.Reconstruct(gibbsStart, rbm.Option.OrderOfGibbsSampling, rbm.Option.UseMean)

		// keep recostructed visible
		if rbm.Option.UsePersistent {
			rbm.PersistentVisibleUnits[persistentIndex] = reconstructedVisible
		}

		// pre-computation that is used in gradient computation
		p_h_given_v1 := rbm.P_H_Given_V_Batch(v)
		p_h_given_v2 := rbm.P_H_Given_V_Batch(reconstructedVisible)

		// Gompute gradient of W
		for i := 0; i < rbm.NumHiddenUnits; i++ {
			for j := 0; j < rbm.NumVisibleUnits; j++ {
				gradW[i][j] += p_h_given_v1[i]*v[j] - p_h_given_v2[i]*reconstructedVisible[j]
			}
		}

		// Gompute gradient of B
		for j := 0; j < rbm.NumVisibleUnits; j++ {
			gradB[j] += v[j] - reconstructedVisible[j]
		}

		// Gompute gradient of C
		for i := 0; i < rbm.NumHiddenUnits; i++ {
			gradC[i] += p_h_given_v1[i] - p_h_given_v2[i]
		}
	}

	return rbm.normalizeGradBySizeOfBatch(gradW, gradB, gradC, len(data))
}

func (rbm *GBRBM) normalizeGradBySizeOfBatch(gradW [][]float64, gradB, gradC []float64, size int) ([][]float64, []float64, []float64) {
	for i := 0; i < rbm.NumHiddenUnits; i++ {
		for j := 0; j < rbm.NumVisibleUnits; j++ {
			gradW[i][j] /= float64(size)
		}
	}
	for j := 0; j < rbm.NumVisibleUnits; j++ {
		gradB[j] /= float64(size)
	}
	for i := 0; i < rbm.NumHiddenUnits; i++ {
		gradC[i] /= float64(size)
	}
	return gradW, gradB, gradC
}

func (rbm *GBRBM) UnSupervisedMiniBatchUpdate(batch [][]float64, epoch, miniBatchIndex int) {
	gradW, gradB, gradC := rbm.Gradient(batch, miniBatchIndex)

	momentum := 0.5
	if epoch > 5 {
		momentum = 0.7
	}

	// Update W
	for i := 0; i < rbm.NumHiddenUnits; i++ {
		for j := 0; j < rbm.NumVisibleUnits; j++ {
			grad := momentum*rbm.GradW[i][j] + rbm.Option.LearningRate*gradW[i][j]
			rbm.W[i][j] += grad
			if rbm.Option.L2Regularization {
				rbm.W[i][j] *= (1.0 - rbm.Option.RegularizationRate)
			}
			rbm.GradW[i][j] = grad
		}
	}

	// Update B
	for j := 0; j < rbm.NumVisibleUnits; j++ {
		grad := momentum*rbm.GradB[j] + rbm.Option.LearningRate*gradB[j]
		rbm.B[j] += grad
		rbm.GradB[j] = grad
	}

	// Update C
	for i := 0; i < rbm.NumHiddenUnits; i++ {
		grad := momentum*rbm.GradC[i] + rbm.Option.LearningRate*gradC[i]
		rbm.C[i] += grad
		rbm.GradC[i] = grad
	}
}

// Train performs Contrastive divergense learning algorithm to train GBRBM.
// The alrogithm is based on (mini-batch) Stochastic Gradient Ascent.
func (rbm *GBRBM) Train(data [][]float64, option TrainingOption) error {
	rbm.Option = option
	opt := nnet.BaseTrainingOption{
		Epoches:       rbm.Option.Epoches,
		MiniBatchSize: rbm.Option.MiniBatchSize,
		Monitoring:    rbm.Option.Monitoring,
	}

	// Peistent Contrastive learning
	if rbm.Option.UsePersistent {
		rbm.PersistentVisibleUnits = nnet.MakeMatrix(len(data), len(data[0]))
		copy(rbm.PersistentVisibleUnits, data)
	}

	s := nnet.NewTrainer(opt)
	return s.UnSupervisedMiniBatchTrain(rbm, data)
}
